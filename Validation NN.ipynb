{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# custom percentile function to exactly replicate matlab\n",
    "def quantile(x,q):\n",
    "    n = len(x)\n",
    "    y = np.sort(x)\n",
    "    return(np.interp(q, np.linspace(1/(2*n), (2*n-1)/(2*n), n), y))\n",
    "\n",
    "def prctile(x,p):\n",
    "    return(quantile(x,np.array(p)/100))\n",
    "# discussion: https://stackoverflow.com/questions/24764966/numpy-percentile-function-different-from-matlabs-percentile-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load GKX data \n",
    "data_stocks_dir = './data_stocks/'\n",
    "micro = pd.read_pickle(data_stocks_dir + 'returns_chars_panel_raw.pkl') # use dropbox links to download this\n",
    "macro = pd.read_pickle(data_stocks_dir + 'macro_timeseries.pkl')\n",
    "df = pd.merge(micro,macro,on='date',how='left',suffixes=['','_macro']) # include macro predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient with Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [03:42<00:00,  5.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 28s, sys: 23.6 s, total: 3min 51s\n",
      "Wall time: 3min 43s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# clean data\n",
    "wm,x,ret,rf = [],[],[],[] # lists holding data for each period\n",
    "mean_chars = [] # for debug\n",
    "# for year in tqdm(range(1974,2016)): # using data from 1974 as in BSV, but longer sample\n",
    "for year in tqdm(range(1974,2016)): # using data from 1974 as in BSV, but longer sample\n",
    "    for month in range(1,13):\n",
    "        dta = df[(df['date'].dt.year==year) & (df['date'].dt.month==month)] # data for this period\n",
    "        \n",
    "        cols = ['mvel1','bm','mom12m','ret','rfree']\n",
    "        dta = dta[cols].dropna()\n",
    "                \n",
    "        # cleaning: remove size below 0.2 percentile and negative beme\n",
    "        min_me = prctile(dta['mvel1'],0.2)\n",
    "        dta = dta[(dta['mvel1']>=min_me) & (dta['bm']>=0)]\n",
    "        \n",
    "        # get value weights for bsv policy\n",
    "        mv = dta['mvel1'].values.reshape(-1,1)\n",
    "        wm.append(mv/np.sum(mv))\n",
    "        \n",
    "        # get normalised characteristics for policy\n",
    "        sz = np.log(dta['mvel1'])\n",
    "        btm = np.log(1+dta['bm'])\n",
    "        mom = dta['mom12m']\n",
    "        char = np.vstack([sz,btm,mom]).T\n",
    "        mean_chars.append(np.mean(char,axis=0))\n",
    "        char -= np.mean(char,axis=0)\n",
    "        char /= np.std(char,axis=0)\n",
    "        x.append(char)\n",
    "        \n",
    "        # get returns\n",
    "        ret.append(dta['ret'].values.reshape(-1,1))\n",
    "        \n",
    "        # risk free rate\n",
    "        rf.append(dta['rfree'].mean())\n",
    "        \n",
    "plt.plot([len(r) for r in ret])\n",
    "plt.xlabel('month')\n",
    "plt.ylabel('number of stocks')\n",
    "del df, micro, macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network(layers):\n",
    "    # random initialization of parameters of neural network\n",
    "    np.random.seed(108)\n",
    "    pars = [] # conveniently to store parameters in a list of dictionaries\n",
    "    for j in range(1,len(layers)): # loop over layers\n",
    "        dictionary = {'weight' : np.random.randn(layers[j],layers[j-1]), # initialize matrix W(l) of weights\n",
    "                      'bias' : np.random.randn(layers[j],1)} # initialize vector b(l) of biases\n",
    "        pars.append(dictionary)\n",
    "    return pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unstack_theta(original_theta, layers):\n",
    "    flattened_theta = []\n",
    "    for i in range(len(layers) - 1):\n",
    "        flattened_theta.extend(np.array(original_theta[i]['weight']).flatten())\n",
    "        flattened_theta.extend(np.array(original_theta[i]['bias']).flatten())\n",
    "    return flattened_theta\n",
    "\n",
    "def stack_theta(flattened_theta, layers):\n",
    "    original_theta = []\n",
    "    index = 0\n",
    "    for i in range(len(layers) - 1):\n",
    "        retrieve_len = layers[i+1] * layers[i]\n",
    "        weight_list = np.array(flattened_theta[index:index + retrieve_len]).reshape(layers[i+1], layers[i])\n",
    "        index += retrieve_len\n",
    "        retrieve_len = layers[i+1] * 1\n",
    "        bias_list = np.array(flattened_theta[index:index + retrieve_len]).reshape(layers[i+1], 1)\n",
    "        index += retrieve_len\n",
    "        original_theta.append({'weight': weight_list, 'bias': bias_list})\n",
    "    return original_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X,pars):\n",
    "    \n",
    "    # make lists to store elements of the graph in memory\n",
    "    Zs,Hs = [],[]\n",
    "    \n",
    "    # initialize inputs\n",
    "    H = X.T\n",
    "    \n",
    "    # loop over layers\n",
    "    for j in range(len(pars)):\n",
    "        \n",
    "        # get parameters for this layer      \n",
    "        W = pars[j]['weight']\n",
    "        b = pars[j]['bias']\n",
    "                \n",
    "        # activations and output\n",
    "        Z = np.matmul(W,H) + b \n",
    "        H = Z if j+1 == len(pars) else Z*(Z>0)\n",
    "         \n",
    "        # save to list\n",
    "        Zs.append(Z)\n",
    "        Hs.append(H)\n",
    "    \n",
    "    return Zs,Hs\n",
    "    # return Hs[-1].flatten()\n",
    "# NB: we could also explicitly return predictions but this is sufficient  and cleaner\n",
    "\n",
    "def backprop(Zs,Hs,X,pars,delta_y=1e-5):\n",
    "    \n",
    "    # setup list for gradients\n",
    "    grads = []\n",
    "    \n",
    "    # data size\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # loop over layers\n",
    "    for j in range(len(pars))[::-1]:\n",
    "        Z = Zs[j]\n",
    "        H = Hs[j]\n",
    "        W = pars[j]['weight']\n",
    "        \n",
    "        # get activations gradient\n",
    "        dZ = (H - (H - delta_y)) if j+1 == len(pars) else (Z > 0) * dH\n",
    "        # print('dZ shape', dZ.shape)\n",
    "        \n",
    "        # get input from previous layer\n",
    "        H_back = Hs[j-1] if j>0 else X.T\n",
    "        # print('H_back shape', H_back.shape)\n",
    "                \n",
    "        # get parameter gradients\n",
    "        dZdW_list = []\n",
    "        dZdb_list = []\n",
    "        for k in range(m):\n",
    "            # print('dZ[:, k] shape', dZ[:,k:k+1].shape)\n",
    "            # print('H_back[:,k] shape', H_back[:,k:k+1].shape)\n",
    "            dW = np.matmul(dZ[:,k:k+1],H_back[:,k:k+1].T)\n",
    "            # print('dW shape', dW.shape)\n",
    "            db = dZ[:, k:k+1]\n",
    "            # db = np.sum(dZ,axis=1,keepdims=True) / m\n",
    "            # print('db shape', db.shape)\n",
    "            dZdW = np.divide(np.zeros_like(dW) + delta_y, dW, out=np.zeros_like(dW), where=dW!=0)\n",
    "            dZdb = np.divide(np.zeros_like(db) + delta_y, db, out=np.zeros_like(db), where=db!=0)\n",
    "            dZdW_list.append(dZdW)\n",
    "            dZdb_list.append(dZdb)\n",
    "        \n",
    "        # save to list\n",
    "        grads.append({'weight' : dZdW_list, 'bias' : dZdb_list})\n",
    "        grads = grads[::-1]\n",
    "        \n",
    "        # move to next layer\n",
    "        if j>0: dH = np.matmul(W.T,dZ)\n",
    "\n",
    "    flattened_theta_list = []\n",
    "    for i in range(m):\n",
    "        flattened_theta = []\n",
    "        for j in range(len(pars)):\n",
    "            flattened_theta.extend(np.array(grads[j]['weight'][i]).flatten())\n",
    "            flattened_theta.extend(np.array(grads[j]['bias'][i]).flatten())\n",
    "        flattened_theta_list.append(flattened_theta)  \n",
    "    return flattened_theta_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# policy function\n",
    "def policy(theta,x,wm, layers):\n",
    "    # theta_ = theta.reshape(-1,1)\n",
    "    w = []\n",
    "    for t in range(len(x)):\n",
    "        _, Hs = forward(x[t],stack_theta(theta, layers))\n",
    "        theta_ = Hs[-1].flatten().reshape(-1, 1)\n",
    "        w.append(wm[t] + theta_ / len(wm[t])) # portfolio weight\n",
    "        # print(len(wm[t] + np.matmul(x[t],theta_) / len(wm[t])), type(wm[t] + np.matmul(x[t],theta_) / len(wm[t])))\n",
    "    return w\n",
    "\n",
    "def policy_copy(theta,x,wm):\n",
    "    theta_ = theta.reshape(-1,1) \n",
    "    w = []\n",
    "    for t in range(len(x)):\n",
    "        w.append(wm[t] + np.matmul(x[t],theta_) / len(wm[t])) # portfolio weight\n",
    "        # print(len(wm[t] + np.matmul(x[t],theta_) / len(wm[t])), type(wm[t] + np.matmul(x[t],theta_) / len(wm[t])))\n",
    "    return w\n",
    "\n",
    "# value function\n",
    "def value(ret,w,gamma=5):\n",
    "    u = []\n",
    "    for t in range(len(ret)):\n",
    "        retp = np.sum(w[t]*ret[t]) # portfolio return\n",
    "        if gamma == 1:\n",
    "            u.append(np.log(1+retp))\n",
    "        else:\n",
    "            u.append((1+retp)**(1-gamma) / (1-gamma))\n",
    "    return np.mean(u)\n",
    "\n",
    "def portfolio_returns(ret,w):\n",
    "    retps = []\n",
    "    for t in range(len(ret)):\n",
    "        retp = np.sum(w[t]*ret[t]) # portfolio return\n",
    "        retps.append(retp)\n",
    "    return np.array(retps)\n",
    "\n",
    "# analytical gradient \n",
    "def grad_analytical_NN(theta,w,x,wm,ret,gamma=5):\n",
    "    grads = []\n",
    "    for t in range(len(ret)):\n",
    "        retp = np.sum(w[t]*ret[t]) \n",
    "        m = (1+retp)**(-gamma)\n",
    "        z = m * ret[t] / len(ret[t])\n",
    "        pars = stack_theta(theta, layers)\n",
    "        Zs, Hs = forward(x[t], pars)\n",
    "        backprop_grad = backprop(Zs, Hs, x[t], pars)\n",
    "        grads.append(np.matmul(z.T,np.array(backprop_grad)))\n",
    "    return np.mean(np.vstack(grads),axis=0)\n",
    "\n",
    " # numeric gradient\n",
    "def grad(theta,w,x,wm,ret,layers,h=1e-5):\n",
    "    dtheta_num = []\n",
    "    for i in range(len(theta)):\n",
    "        h_vec = np.zeros_like(theta) # perturb in direction i\n",
    "        h_vec[i] = h\n",
    "        up = value(ret,policy(theta + h_vec,x,wm, layers))\n",
    "        down = value(ret,policy(theta - h_vec,x,wm, layers))\n",
    "        dtheta_num.append((up - down) / (2*h)) # numerical grad\n",
    "    return np.array(dtheta_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup adam\n",
    "# beta1, beta2, epsilon = 0.99, 0.999, 1e-7 \n",
    "# v, s = 0,0  \n",
    "#  1.learning rate\n",
    "# alpha = 1e-2\n",
    "\n",
    "# split data\n",
    "n_train = 240 # 30 years of training data, ~12 years of test data\n",
    "n_val = 120 # 30 years of training data, ~12 years of test data\n",
    "wm_train, x_train, ret_train, rf_train = wm[0:n_train], x[0:n_train], ret[0:n_train], rf[0:n_train]\n",
    "wm_val, x_val, ret_val, rf_val = wm[n_train:n_train+n_val], x[n_train:n_train+n_val], ret[n_train:n_train+n_val], rf[n_train:n_train+n_val]\n",
    "wm_test, x_test, ret_test, rf_test = wm[n_train+n_val:], x[n_train+n_val:], ret[n_train+n_val:], rf[n_train+n_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(variable_num , hidden_layers ,batch_size, alpha , n_epochs):\n",
    "    # setup adam\n",
    "    beta1, beta2, epsilon = 0.99, 0.999, 1e-7 \n",
    "    v, s = 0,0  \n",
    "    xs, wms, rets = [], [], []\n",
    "    j=0\n",
    "    while j<n_train:\n",
    "        start, end = j, min(j+batch_size,n_train)\n",
    "        xs.append(x_train[start:end])\n",
    "        wms.append(wm_train[start:end])\n",
    "        rets.append(ret_train[start:end])\n",
    "        j+=batch_size\n",
    "    p = variable_num\n",
    "    layers = [p] + hidden_layers + [1]\n",
    "    theta = unstack_theta(initialize_network(layers), layers)\n",
    "    c = 0 # count updates\n",
    "    \n",
    "    value_pos = 0 # get the position of the items in the value list, used for early stopping in learning curve\n",
    "    \n",
    "    \n",
    "    values = [] # for learning curve\n",
    "    values_for_plot = []\n",
    "    for i in tqdm(range(n_epochs)):\n",
    "        values_for_plot.append(value(ret,policy(theta,x,wm, layers)))\n",
    "        if (i+1)%25 == 0: \n",
    "            # print('epoch %d, theta = %s' %(i+1,str(theta)))\n",
    "            values.append(value(ret,policy(theta,x,wm, layers)))\n",
    "            value_pos += 1 \n",
    "\n",
    "        if len(values) > 1: \n",
    "            if values[-1] < values[-2]:\n",
    "                break \n",
    "        for x_batch,wm_batch,ret_batch in zip(xs,wms,rets):\n",
    "            w = policy(theta,x_batch,wm_batch, layers) # forward\n",
    "            dtheta = grad(theta,w,x_batch,wm_batch,ret_batch, layers) # backward\n",
    "\n",
    "            # adam update\n",
    "            v = beta1 * v + (1-beta1) * dtheta\n",
    "            s = beta2 * s + (1-beta2) * (dtheta**2)\n",
    "            vhat = v / (1-(beta1**(c+1)))\n",
    "            shat = s / (1-(beta2**(c+1)))\n",
    "            adam = vhat / (np.sqrt(shat)+epsilon)\n",
    "            theta += alpha * vhat / (np.sqrt(shat)+epsilon)\n",
    "            c+=1\n",
    "\n",
    "    # print results\n",
    "#     print('\\nSOLUTION: optimal theta')\n",
    "#     print(theta)\n",
    "#     print('max utility')\n",
    "#     print(value(ret,policy(theta,x,wm)))\n",
    "    max_uti = value(ret,policy(theta,x,wm, layers))\n",
    "    \n",
    "#     plot learning curve\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.title('Learning curve')\n",
    "    plt.plot(np.arange(len(values_for_plot)),values_for_plot)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('average utility')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # print performance\n",
    "    w = policy(theta,x_train,wm_train, layers)\n",
    "    retp = portfolio_returns(ret_train,w)\n",
    "    sharpe_train = np.sqrt(12)* (retp-np.array(rf_train) ).mean() / retp.std()\n",
    "    print('\\nSharpe ratio train')\n",
    "    print(sharpe_train)\n",
    "\n",
    "    w = policy(theta,x_val,wm_val, layers)\n",
    "    retp = portfolio_returns(ret_val,w)\n",
    "    sharpe_val = np.sqrt(12)* (retp-np.array(rf_val) ).mean() / retp.std()\n",
    "    print('\\nSharpe ratio validation')\n",
    "    print(sharpe_val)\n",
    "\n",
    "    w = policy(theta,x_test,wm_test, layers)\n",
    "    retp = portfolio_returns(ret_test,w)\n",
    "    sharpe_test = np.sqrt(12)* (retp-np.array(rf_test) ).mean() / retp.std()\n",
    "    print('\\nSharpe ratio test')\n",
    "    print(sharpe_test)\n",
    "    \n",
    "    return theta, max_uti, sharpe_train, sharpe_val, sharpe_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of hyperparameter combination: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# param_grid = {'hidden_layers': [[8, 4],[8],[4], []], 'batch_size': [32, 16, 8], \n",
    "#               'alpha': [1e-3, 1e-2,1e-1], 'n_epochs': [1000]}\n",
    "# below is already run\n",
    "# param_grid = {'hidden_layers': [[8]], 'batch_size': [32, 16, 8], \n",
    "#               'alpha': [1e-2,1e-1], 'n_epochs': [1000]}\n",
    "param_grid = {'hidden_layers': [[8, 4]], 'batch_size': [8], \n",
    "               'alpha': [1e-2,1e-1], 'n_epochs': [1000]}\n",
    "print('Total number of hyperparameter combination:', len(list(ParameterGrid(param_grid))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hyperparam in (list(ParameterGrid(param_grid))):\n",
    "    np.random.seed(193)\n",
    "    theta, max_uti, sharpe_train, sharpe_val, sharpe_test = optimize(variable_num=3, **hyperparam)\n",
    "    hyperparam['sharpe_train'] = sharpe_train\n",
    "    hyperparam['sharpe_val'] = sharpe_val \n",
    "    hyperparam['sharpe_test'] = sharpe_test\n",
    "    sharpes.append(hyperparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>n_epochs</th>\n",
       "      <th>sharpe_train</th>\n",
       "      <th>sharpe_val</th>\n",
       "      <th>sharpe_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>[8]</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.906117</td>\n",
       "      <td>1.302920</td>\n",
       "      <td>0.783429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>16</td>\n",
       "      <td>[8]</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.999838</td>\n",
       "      <td>1.235926</td>\n",
       "      <td>0.722431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>[8]</td>\n",
       "      <td>1000</td>\n",
       "      <td>2.182190</td>\n",
       "      <td>1.223085</td>\n",
       "      <td>0.721017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.10</td>\n",
       "      <td>32</td>\n",
       "      <td>[8]</td>\n",
       "      <td>1000</td>\n",
       "      <td>2.038632</td>\n",
       "      <td>1.662114</td>\n",
       "      <td>1.077424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.10</td>\n",
       "      <td>16</td>\n",
       "      <td>[8]</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.921988</td>\n",
       "      <td>0.857971</td>\n",
       "      <td>0.514578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.10</td>\n",
       "      <td>8</td>\n",
       "      <td>[8]</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.996569</td>\n",
       "      <td>0.828580</td>\n",
       "      <td>0.485515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>[8, 4]</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.850885</td>\n",
       "      <td>1.644918</td>\n",
       "      <td>1.044563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.10</td>\n",
       "      <td>8</td>\n",
       "      <td>[8, 4]</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.458503</td>\n",
       "      <td>0.511443</td>\n",
       "      <td>0.502808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alpha  batch_size hidden_layers  n_epochs  sharpe_train  sharpe_val  \\\n",
       "0   0.01          32           [8]      1000      1.906117    1.302920   \n",
       "1   0.01          16           [8]      1000      1.999838    1.235926   \n",
       "2   0.01           8           [8]      1000      2.182190    1.223085   \n",
       "3   0.10          32           [8]      1000      2.038632    1.662114   \n",
       "4   0.10          16           [8]      1000      0.921988    0.857971   \n",
       "5   0.10           8           [8]      1000      0.996569    0.828580   \n",
       "6   0.01           8        [8, 4]      1000      1.850885    1.644918   \n",
       "7   0.10           8        [8, 4]      1000      0.458503    0.511443   \n",
       "\n",
       "   sharpe_test  \n",
       "0     0.783429  \n",
       "1     0.722431  \n",
       "2     0.721017  \n",
       "3     1.077424  \n",
       "4     0.514578  \n",
       "5     0.485515  \n",
       "6     1.044563  \n",
       "7     0.502808  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(sharpes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sharpe = pd.DataFrame(sharpes)\n",
    "df_sharpe.to_csv('sharpe_001.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86aadfea6bd2c19e0110bbb6a3eaa0c722c56d6ac2467bdf337474bddc4baa95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "86aadfea6bd2c19e0110bbb6a3eaa0c722c56d6ac2467bdf337474bddc4baa95"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}